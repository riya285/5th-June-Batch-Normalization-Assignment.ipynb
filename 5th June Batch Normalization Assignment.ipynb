{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767260e-a77a-4a53-a349-87ffc33bb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1. Theory and Concept \n",
    "\n",
    "1.Explain the concept of batch normalization in the context of artificial neural networks\n",
    "\n",
    "\n",
    "Batch normalization is a technique used in training artificial neural networks to improve the stability and performance of the network by normalizing the inputs to each layer. The main idea behind batch normalization is to transform the input data into a normal distribution with a mean of 0 and a standard deviation of 1. This transformation allows the network to learn more easily and make better predictions.\n",
    "\n",
    "In a neural network, the inputs to each layer are typically scaled and shifted by the weights and biases of the previous layer. This can result in the distributions of the inputs to subsequent layers becoming skewed, leading to unstable gradients and poor convergence. Batch normalization addresses this issue by normalizing the inputs to each layer, reducing the effects of internal covariate shift and improving the generalization ability of the network.\n",
    "\n",
    "Batch normalization is computed separately for each mini-batch of data, which consists of a subset of samples from the training dataset. For each mini-batch, the mean and standard deviation of the inputs are computed, and then the inputs are transformed using the following formula:\n",
    "\n",
    "output = γ(input - μ) / σ + β\n",
    "\n",
    "where γ and β are learned scalars, μ is the mean of the inputs, and σ is the standard deviation of the inputs. The parameters γ and β are learned during training, and they control the scale and shift of the normalized inputs.\n",
    "\n",
    "By normalizing the inputs, batch normalization achieves several benefits. First, it reduces the impact of outliers in the input data, since extreme values are transformed to be closer to the mean. Second, it accelerates the training process, since the gradients no longer explode or vanish during backpropagation. Third, it improves the generalization ability of the network, since the normalization helps the network learn more robust features that are less sensitive to the scale and shift of the input data.\n",
    "\n",
    "Overall, batch normalization is a simple yet powerful technique that significantly improves the performance of deep neural networks. Its ability to reduce the internal covariate shift and improve the stability of the training process makes it a crucial component of modern deep learning architectures.\n",
    "\n",
    "2. Describe the benefits of using batch normalization during trainin\n",
    "\n",
    "Batch Normalization (BN) provides several benefits during the training of neural networks, contributing to improved convergence, stability, and generalization. Here are the key advantages:\n",
    "\n",
    "Accelerated Training Convergence:\n",
    "\n",
    "Batch Normalization helps to stabilize and accelerate the training process. By maintaining the activations within a certain range, it allows for the use of higher learning rates, leading to faster convergence.\n",
    "Mitigation of Internal Covariate Shift:\n",
    "\n",
    "Internal Covariate Shift refers to the change in the distribution of layer inputs during training. Batch Normalization mitigates this issue by normalizing the inputs for each mini-batch. This enables more stable and consistent training by reducing the impact of shifting distributions.\n",
    "Reduced Sensitivity to Initialization:\n",
    "\n",
    "Neural networks are often sensitive to the initial values of their weights. Batch Normalization reduces this sensitivity, allowing for more flexibility in choosing initial weights and making it easier to train deep networks.\n",
    "Regularization Effect:\n",
    "\n",
    "Batch Normalization introduces a slight regularization effect during training. This can reduce the need for other regularization techniques, such as dropout, as BN inherently adds some noise to the training process.\n",
    "Improved Gradient Flow:\n",
    "\n",
    "By normalizing the inputs, Batch Normalization helps maintain a more consistent scale of activations across layers. This improves the flow of gradients during backpropagation, mitigating the vanishing or exploding gradient problems.\n",
    "Facilitation of Deeper Networks:\n",
    "\n",
    "Batch Normalization facilitates the training of deeper networks. As networks become deeper, it becomes challenging to ensure stable and efficient training. Batch Normalization helps by normalizing the inputs at each layer, making it easier to train deep architectures.\n",
    "Better Handling of Non-linearity:\n",
    "\n",
    "Batch Normalization reduces the impact of saturating non-linearities, such as sigmoid or tanh functions, by keeping activations within a reasonable range. This is particularly beneficial in networks with deep architectures.\n",
    "Improved Generalization:\n",
    "\n",
    "BN has been observed to act as a form of implicit regularization, contributing to better generalization on unseen data. This can result in improved performance on validation and test sets.\n",
    "Application to Various Architectures:\n",
    "\n",
    "Batch Normalization is applicable to different types of neural network architectures, including feedforward networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). Its versatility has contributed to its widespread adoption.\n",
    "In summary, Batch Normalization is a crucial technique in the training of neural networks, providing a range of benefits that collectively enhance the efficiency, stability, and generalization of the learning process.\n",
    "\n",
    "\n",
    "3. Discuss the working principle of batch normalization, including the normalization step and the learnable\n",
    "parameters.\n",
    "\n",
    "\n",
    "Sources: towardsdatascience.com (1) analyticsvidhya.com (2) en.wikipedia.org (3) medium.com (4)\n",
    "\n",
    "Batch normalization is a widely used technique in deep learning that helps to improve the stability and speed of training neural networks. The core idea behind batch normalization is to transform the input data into a normal distribution with a mean of 0 and a standard deviation of 1. This transformation allows the network to learn more robust features that are less sensitive to the scale and shift of the input data.\n",
    "\n",
    "The working principle of batch normalization can be broken down into two main steps:\n",
    "\n",
    "Normalization Step: In this step, the input data is subtracted by the mean and divided by the standard deviation for each feature map within a mini-batch. The mean and standard deviation are computed over the mini-batch, and the same parameters are used for all feature maps within the mini-batch. The formula for the normalization step is:\n",
    "z = γ(x - μ) / σ + β\n",
    "\n",
    "where x is the input feature map, μ is the mean of the feature map, σ is the standard deviation of the feature map, γ is the scaling parameter, and β is the shifting parameter. The parameters γ and β are learned during training and are shared across all feature maps within a layer.\n",
    "\n",
    "Learnable Parameters: The learnable parameters in batch normalization are the scaling parameter γ and the shifting parameter β. These parameters are learned by minimizing the loss function of the network, along with the other weights and biases. During training, the gradients of the loss function with respect to γ and β are computed, and the parameters are updated accordingly.\n",
    "The key advantage of batch normalization is that it allows the network to learn more robust features that are less sensitive to the scale and shift of the input data. By transforming the input data into a normal distribution, the network learns features that are invariant to the scale and shift of the input data, which can improve the generalization ability of the network. Additionally, batch normalization can also reduce the internal covariate shift, which can improve the stability and speed of training.\n",
    "\n",
    "In summary, batch normalization is a simple yet powerful technique that can improve the stability and speed of training deep neural networks. By transforming the input data into a normal distribution, batch normalization allows the network to learn more robust features that are less sensitive to the scale and shift of the input data, leading to improved generalization ability and faster convergence.\n",
    "\n",
    "\n",
    "\n",
    "Q2. Implementation:\n",
    "    \n",
    "1.   I have choose CIFAR-10 as my dataset. Here's how I would preprocess the data:\n",
    "\n",
    "a.Load the dataset: First, I'll load the CIFAR-10 dataset using the tf.keras.datasets.cifar10.load_data() function. This function loads the dataset and returns a tuple containing the training data, testing data, and their corresponding labels.\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "b. Normalize pixel values: Next, I'll normalize the pixel values of the images to be between 0 and 1. This is necessary because the input layer of a neural network expects the inputs to be normalized. I'll use the tf.image.per_image_standardization function to normalize the pixel values.\n",
    "\n",
    "X_train = tf.image.per_image_standardization(X_train)\n",
    "X_test = tf.image.per_image_standardization(X_test)\n",
    "\n",
    "c. Split the data: Since I'm using transfer learning, I'll split the data into a training set and a validation set. I'll use the tf.keras.preprocessing.sequence.train_test_split function to split the data.\n",
    "\n",
    "X_train, X_val, y_train, y_val = tf.keras.preprocessing.sequence.train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "d. One-hot encode labels: Finally, I'll one-hot encode the labels using the tf.keras.utils.to_categorical function. This is necessary because the output layer of a neural network expects the targets to be one-hot encoded.\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_val = tf.keras.utils.to_categorical(y_val)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "Now that the data is preprocessed, I can define my model architecture and start training the model..</s><s>using System; using System.Collections.Generic; using System.ComponentModel; using System.Data; using System.Drawing; using System.Linq; using System.Text; using System.Threading.Tasks; using System.Windows.Forms;\n",
    "\n",
    "namespace OrderSystem { public partial class OrderDetail : Form { private OrderItem currentOrderItem; public OrderDetail() { InitializeComponent(); LoadData(); }\n",
    "                                                                 \n",
    "  \n",
    "    public OrderDetail(OrderItem orderItem)\n",
    "    {\n",
    "        InitializeComponent();\n",
    "        currentOrderItem = orderItem;\n",
    "        LoadData();\n",
    "    }\n",
    "\n",
    "    private void LoadData()\n",
    "    {\n",
    "        label1.Text = currentOrderItem.name;\n",
    "        label2.Text = currentOrderItem.description;\n",
    "        label3.Text = currentOrderItem.quantity.ToString();\n",
    "        label4.Text = currentOrderItem.price.ToString();\n",
    "        label5.Text = currentOrderItem.totalPrice.ToString();\n",
    "        label6.Text = currentOrderItem.tableNo.ToString();\n",
    "        label7.Text = currentOrderItem.dateTime.ToString();\n",
    "        label8.Text = currentOrderItem.status;\n",
    "    }\n",
    "\n",
    "    private void OrderDetail_Load(object sender, EventArgs e)\n",
    "    {\n",
    "\n",
    "    }\n",
    "}                                                                 \n",
    "                                                                 \n",
    "   \n",
    "Q2. Implement a simple feedforward neural network using any deep learning framework/library (e.g.,Tensorflow, PyTorch).\n",
    "                       \n",
    "                       \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)  # input layer (784) -> hidden layer (128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # hidden layer (128) -> output layer (10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # activation function for hidden layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "                       \n",
    "\n",
    "      Here's an example of how to do that using the MNIST dataset:\n",
    "                       \n",
    " from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss.item()))\n",
    "                       \n",
    "                       \n",
    "Q3. Train the neural network on the chosen dataset without using batch normalization.\n",
    "\n",
    " \n",
    " import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess the MNIST dataset (assuming you've done the preprocessing steps)\n",
    "# ...\n",
    "\n",
    "# Build the feedforward neural network model without batch normalization\n",
    "model_no_bn = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_no_bn.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model_no_bn.summary()\n",
    "\n",
    "# Train the model without batch normalization\n",
    "model_no_bn.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))\n",
    "\n",
    "                       \n",
    "Q4. Implement batch normalization layers in the neural network and train the model again\n",
    "                      \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess the MNIST dataset (assuming you've done the preprocessing steps)\n",
    "# ...\n",
    "\n",
    "# Build the feedforward neural network model with batch normalization\n",
    "model_with_bn = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    layers.Dense(128),\n",
    "    layers.BatchNormalization(),  # Batch normalization layer\n",
    "    layers.Activation('relu'),    # ReLU activation after batch normalization\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_with_bn.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model_with_bn.summary()\n",
    "\n",
    "# Train the model with batch normalization\n",
    "model_with_bn.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))\n",
    "\n",
    "                       \n",
    "Q5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and\n",
    "without batch normalization\n",
    "\n",
    "\n",
    "  # Evaluate the model without batch normalization\n",
    "eval_no_bn = model_no_bn.evaluate(test_images, test_labels)\n",
    "print(f\"Model without Batch Normalization - Loss: {eval_no_bn[0]}, Accuracy: {eval_no_bn[1]}\")\n",
    "\n",
    "# Evaluate the model with batch normalization\n",
    "eval_with_bn = model_with_bn.evaluate(test_images, test_labels)\n",
    "print(f\"Model with Batch Normalization - Loss: {eval_with_bn[0]}, Accuracy: {eval_with_bn[1]}\")\n",
    "\n",
    "                       \n",
    "    \n",
    "                       \n",
    "# Train the model without batch normalization\n",
    "history_no_bn = model_no_bn.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Train the model with batch normalization\n",
    "history_with_bn = model_with_bn.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Access training and validation metrics\n",
    "train_acc_no_bn = history_no_bn.history['accuracy']\n",
    "val_acc_no_bn = history_no_bn.history['val_accuracy']\n",
    "\n",
    "train_acc_with_bn = history_with_bn.history['accuracy']\n",
    "val_acc_with_bn = history_with_bn.history['val_accuracy']\n",
    "\n",
    "# Plotting example\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc_no_bn, label='No Batch Normalization')\n",
    "plt.plot(train_acc_with_bn, label='With Batch Normalization')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_acc_no_bn, label='No Batch Normalization')\n",
    "plt.plot(val_acc_with_bn, label='With Batch Normalization')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "Q6.  Discuss the impact of batch normalization on the training process and the performance of the neural\n",
    "network.\n",
    "                       \n",
    "\n",
    "Batch Normalization (BN) has several impacts on the training process and the performance of neural networks:\n",
    "\n",
    "Improved Training Stability:\n",
    "\n",
    "BN helps mitigate the internal covariate shift, ensuring that the distribution of inputs to each layer remains more stable throughout training. This stabilizes the learning process, making it less sensitive to the choice of hyperparameters and weight initialization.\n",
    "Faster Convergence:\n",
    "\n",
    "The normalization of inputs allows for more stable and efficient training. BN often leads to faster convergence during training, reducing the number of epochs required to reach a certain level of performance.\n",
    "Higher Learning Rates:\n",
    "\n",
    "BN allows the use of higher learning rates during training without the risk of divergence. This speeds up the training process and can lead to better generalization.\n",
    "Reduction of Dependency on Initialization:\n",
    "\n",
    "Neural networks are sensitive to the initial values of weights. Batch Normalization reduces this sensitivity, making it easier to initialize weights in a way that facilitates learning.\n",
    "Regularization Effect:\n",
    "\n",
    "BN introduces a slight regularization effect, reducing the need for additional regularization techniques such as dropout. This can prevent overfitting and improve the model's ability to generalize to unseen data.\n",
    "Facilitation of Deeper Networks:\n",
    "\n",
    "Training very deep networks can be challenging due to issues like vanishing or exploding gradients. BN helps address these challenges, enabling the training of deeper and more complex architectures.\n",
    "Removal of the Need for Biases:\n",
    "\n",
    "Batch Normalization reduces the reliance on biases in neural network layers. Biases are less crucial because BN centers the data, and it helps ensure that the network is not overly dependent on biases for learning.\n",
    "Applicability Across Network Types:\n",
    "\n",
    "Batch Normalization is applicable to various types of neural networks, including feedforward networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). Its versatility makes it a widely used technique.\n",
    "However, it's essential to note that while Batch Normalization provides significant benefits in many cases, it might not always be the best choice. In certain scenarios, such as in very small datasets or specific network architectures, the introduction of batch normalization may not yield significant improvements and could even be counterproductive. Therefore, it's crucial to experiment and validate its impact based on the specific characteristics of the task and dataset. Additionally, in some cases, newer normalization techniques like Layer Normalization or Group Normalization might be considered as alternatives to Batch Normalization.\n",
    "                       \n",
    "\n",
    "                       Q3. Experimentation and analysis.     \n",
    "                       \n",
    "                       \n",
    "                       1. Experiment with different batch sizes and observe the effect on the training dynamics and model\n",
    "performancer.                 \n",
    "                       \n",
    "                       \n",
    "Experimentation with Different Batch Sizes:\n",
    "1. Varying Batch Sizes:\n",
    "Experimenting with different batch sizes can provide insights into how the choice of batch size affects the training dynamics and model performance. Here's an outline of how you might approach this experiment:\n",
    "                       \n",
    "  \n",
    "                       # Assuming you have a model architecture defined (e.g., model_with_bn) and the dataset loaded\n",
    "\n",
    "batch_sizes = [32, 64, 128, 256]  # Try different batch sizes\n",
    "epochs = 10\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    model = model_with_bn  # Use the same model for each experiment\n",
    "    \n",
    "    # Train the model with the current batch size\n",
    "    history = model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(test_images, test_labels))\n",
    "    \n",
    "    # Evaluate and print the results\n",
    "    eval_result = model.evaluate(test_images, test_labels)\n",
    "    print(f\"\\nBatch Size: {batch_size}\")\n",
    "    print(f\"Validation Loss: {eval_result[0]}, Validation Accuracy: {eval_result[1]}\")\n",
    "\n",
    "    # Plot training dynamics (optional)\n",
    "    plt.plot(history.history['val_accuracy'], label=f'Batch Size {batch_size}')\n",
    "\n",
    "plt.title('Validation Accuracy with Different Batch Sizes')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "     2. Analysis:\n",
    "Effect on Training Dynamics: Smaller batch sizes often result in more frequent updates to the model's weights, leading to increased training dynamics. Larger batch sizes, on the other hand, might provide a smoother optimization trajectory but might also get stuck in local minima.\n",
    "\n",
    "Effect on Model Performance: The impact on model performance depends on various factors. Smaller batch sizes may introduce more noise but could lead to better generalization. Larger batch sizes may provide more accurate gradient estimates but could lead to overfitting.\n",
    "\n",
    "Computational Efficiency: Larger batch sizes often lead to more efficient computation, especially on hardware like GPUs, due to parallelization. Smaller batch sizes might be computationally less efficient but could lead to better generalization.\n",
    "                       \n",
    "                       \n",
    "2. Discuss the advantages and potential limitations of batch normalization in improving the training of\n",
    "neural networks.\n",
    "                       \n",
    "        \n",
    "Advantages and Potential Limitations of Batch Normalization:\n",
    "Advantages:\n",
    "Stability during Training:\n",
    "\n",
    "Batch Normalization helps stabilize the training process by maintaining consistent input distributions across layers, reducing internal covariate shift.\n",
    "Faster Convergence:\n",
    "\n",
    "It enables faster convergence during training by allowing the use of higher learning rates and reducing the number of training epochs needed.\n",
    "Regularization:\n",
    "\n",
    "Batch Normalization acts as a form of regularization, reducing the need for other regularization techniques like dropout.\n",
    "Facilitation of Deeper Networks:\n",
    "\n",
    "It addresses challenges associated with training deep networks by mitigating vanishing or exploding gradients.\n",
    "Reduced Dependency on Weight Initialization:\n",
    "\n",
    "Batch Normalization reduces the sensitivity of neural networks to the choice of weight initialization.\n",
    "Potential Limitations:\n",
    "Effectiveness in Small Datasets:\n",
    "\n",
    "Batch Normalization might not provide significant benefits in small datasets, and in some cases, it could even be counterproductive.\n",
    "Batch Size Sensitivity:\n",
    "\n",
    "The effectiveness of Batch Normalization can be sensitive to the choice of batch size. Smaller batch sizes might introduce noise, and larger batch sizes might not provide the same regularization effect.\n",
    "Influence on Training Dynamics:\n",
    "\n",
    "In some cases, Batch Normalization might alter the training dynamics in ways that could affect the convergence behavior, especially when applied to very small or very large batch sizes.\n",
    "Not Always Necessary:\n",
    "\n",
    "For some simple tasks or architectures, the overhead introduced by Batch Normalization might not be necessary, and the model could perform well without it.\n",
    "Not Always Applicable:\n",
    "\n",
    "Batch Normalization may not be applicable in certain scenarios, such as online learning or reinforcement learning, where batch statistics are not well-defined.\n",
    "In conclusion, while Batch Normalization has proven to be a valuable tool in the training of neural networks, its effectiveness can depend on various factors, and it's essential to experiment and analyze its impact based on the specific characteristics of the task and dataset at hand.\n",
    "                       \n",
    "                       \n",
    "                       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
